{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary and residual imports\n",
    "import nltk\n",
    "from nltk.corpus import udhr\n",
    "import keras\n",
    "from nltk import FreqDist\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import regex\n",
    "import nltk.tokenize.casual\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset: (from https://huggingface.co/datasets/versae/bibles)\n",
    "# https://stackoverflow.com/questions/39263929/how-can-i-read-tar-gz-file-using-pandas-read-csv-with-gzip-compression-option\n",
    "df = pd.read_csv('books_labels.tar.gz', compression='gzip', header=0, sep=',', quotechar='\"', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Spanish texts\n",
    "# https://stackoverflow.com/questions/17424182/extracting-all-rows-from-pandas-dataframe-that-have-certain-value-in-a-specific\n",
    "#spanish_df = df[df['language'] == 'SPA']['text']\n",
    "#spanish_list = spanish_df.to_list() # https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://ioflood.com/blog/dataframe-to-list-pandas/%23:~:text%3DIt%2520is%2520utilized%2520with%2520the,tolist()%2520.%26text%3DIn%2520the%2520example%2520above%252C%2520we,convert%2520it%2520into%2520a%2520list.&ved=2ahUKEwiP3deMnLSFAxVD4ckDHUo7BsEQFnoECA4QAw&usg=AOvVaw3GgfIVdsVo9Dxul02uata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>books_labels.csv</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>codebook</th>\n",
       "      <th>language</th>\n",
       "      <th>book_file_name</th>\n",
       "      <th>file_name_translation</th>\n",
       "      <th>source</th>\n",
       "      <th>year</th>\n",
       "      <th>genre</th>\n",
       "      <th>genre-multilabel</th>\n",
       "      <th>testament</th>\n",
       "      <th>division</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2972645</th>\n",
       "      <td>0.0</td>\n",
       "      <td>b.1CH.001.001</td>\n",
       "      <td>Adán, Set, Enós,</td>\n",
       "      <td>1CH</td>\n",
       "      <td>SPA</td>\n",
       "      <td>1CH_Biblia del Jubileo (JBS)</td>\n",
       "      <td>Biblia del Jubileo (JBS)</td>\n",
       "      <td>biblegateway</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>historical</td>\n",
       "      <td>historical</td>\n",
       "      <td>old</td>\n",
       "      <td>Historical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972656</th>\n",
       "      <td>11.0</td>\n",
       "      <td>b.1CH.001.002</td>\n",
       "      <td>Cainán, Mahalaleel, Jared,</td>\n",
       "      <td>1CH</td>\n",
       "      <td>SPA</td>\n",
       "      <td>1CH_Biblia del Jubileo (JBS)</td>\n",
       "      <td>Biblia del Jubileo (JBS)</td>\n",
       "      <td>biblegateway</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>historical</td>\n",
       "      <td>historical</td>\n",
       "      <td>old</td>\n",
       "      <td>Historical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972667</th>\n",
       "      <td>22.0</td>\n",
       "      <td>b.1CH.001.003</td>\n",
       "      <td>Enoc, Matusalén, Lamec,</td>\n",
       "      <td>1CH</td>\n",
       "      <td>SPA</td>\n",
       "      <td>1CH_Biblia del Jubileo (JBS)</td>\n",
       "      <td>Biblia del Jubileo (JBS)</td>\n",
       "      <td>biblegateway</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>historical</td>\n",
       "      <td>historical</td>\n",
       "      <td>old</td>\n",
       "      <td>Historical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972678</th>\n",
       "      <td>33.0</td>\n",
       "      <td>b.1CH.001.004</td>\n",
       "      <td>Noé, Sem, Cam, y Jafet.</td>\n",
       "      <td>1CH</td>\n",
       "      <td>SPA</td>\n",
       "      <td>1CH_Biblia del Jubileo (JBS)</td>\n",
       "      <td>Biblia del Jubileo (JBS)</td>\n",
       "      <td>biblegateway</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>historical</td>\n",
       "      <td>historical</td>\n",
       "      <td>old</td>\n",
       "      <td>Historical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972689</th>\n",
       "      <td>44.0</td>\n",
       "      <td>b.1CH.001.005</td>\n",
       "      <td>Los hijos de Jafet: Gomer, Magog, Madai, Javán...</td>\n",
       "      <td>1CH</td>\n",
       "      <td>SPA</td>\n",
       "      <td>1CH_Biblia del Jubileo (JBS)</td>\n",
       "      <td>Biblia del Jubileo (JBS)</td>\n",
       "      <td>biblegateway</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>historical</td>\n",
       "      <td>historical</td>\n",
       "      <td>old</td>\n",
       "      <td>Historical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         books_labels.csv             id  \\\n",
       "2972645               0.0  b.1CH.001.001   \n",
       "2972656              11.0  b.1CH.001.002   \n",
       "2972667              22.0  b.1CH.001.003   \n",
       "2972678              33.0  b.1CH.001.004   \n",
       "2972689              44.0  b.1CH.001.005   \n",
       "\n",
       "                                                      text codebook language  \\\n",
       "2972645                                   Adán, Set, Enós,      1CH      SPA   \n",
       "2972656                         Cainán, Mahalaleel, Jared,      1CH      SPA   \n",
       "2972667                            Enoc, Matusalén, Lamec,      1CH      SPA   \n",
       "2972678                            Noé, Sem, Cam, y Jafet.      1CH      SPA   \n",
       "2972689  Los hijos de Jafet: Gomer, Magog, Madai, Javán...      1CH      SPA   \n",
       "\n",
       "                       book_file_name     file_name_translation        source  \\\n",
       "2972645  1CH_Biblia del Jubileo (JBS)  Biblia del Jubileo (JBS)  biblegateway   \n",
       "2972656  1CH_Biblia del Jubileo (JBS)  Biblia del Jubileo (JBS)  biblegateway   \n",
       "2972667  1CH_Biblia del Jubileo (JBS)  Biblia del Jubileo (JBS)  biblegateway   \n",
       "2972678  1CH_Biblia del Jubileo (JBS)  Biblia del Jubileo (JBS)  biblegateway   \n",
       "2972689  1CH_Biblia del Jubileo (JBS)  Biblia del Jubileo (JBS)  biblegateway   \n",
       "\n",
       "           year       genre genre-multilabel testament    division  \n",
       "2972645  2000.0  historical       historical       old  Historical  \n",
       "2972656  2000.0  historical       historical       old  Historical  \n",
       "2972667  2000.0  historical       historical       old  Historical  \n",
       "2972678  2000.0  historical       historical       old  Historical  \n",
       "2972689  2000.0  historical       historical       old  Historical  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select Spanish texts\n",
    "# https://stackoverflow.com/questions/17424182/extracting-all-rows-from-pandas-dataframe-that-have-certain-value-in-a-specific\n",
    "spanish_df = df[df['language'] == 'SPA']\n",
    "portuguese_df = df[df['language'] == 'POR']\n",
    "spanish_df.sort_values(by=['file_name_translation', 'id']).head(5)\n",
    "#spanish_list = spanish_df.to_list() # https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://ioflood.com/blog/dataframe-to-list-pandas/%23:~:text%3DIt%2520is%2520utilized%2520with%2520the,tolist()%2520.%26text%3DIn%2520the%2520example%2520above%252C%2520we,convert%2520it%2520into%2520a%2520list.&ved=2ahUKEwiP3deMnLSFAxVD4ckDHUo7BsEQFnoECA4QAw&usg=AOvVaw3GgfIVdsVo9Dxul02uata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>books_labels.csv</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>codebook</th>\n",
       "      <th>language</th>\n",
       "      <th>book_file_name</th>\n",
       "      <th>file_name_translation</th>\n",
       "      <th>source</th>\n",
       "      <th>year</th>\n",
       "      <th>genre</th>\n",
       "      <th>genre-multilabel</th>\n",
       "      <th>testament</th>\n",
       "      <th>division</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1839923</th>\n",
       "      <td>0.0</td>\n",
       "      <td>b.1CH.001.001</td>\n",
       "      <td>ADÃO, Sete, Enos,</td>\n",
       "      <td>1CH</td>\n",
       "      <td>POR</td>\n",
       "      <td>1CH_SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGID...</td>\n",
       "      <td>SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGIDA FI...</td>\n",
       "      <td>zefania</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>historical</td>\n",
       "      <td>historical</td>\n",
       "      <td>old</td>\n",
       "      <td>Historical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839924</th>\n",
       "      <td>1.0</td>\n",
       "      <td>b.1CH.001.002</td>\n",
       "      <td>Cainã, Maalaleel, Jerede,</td>\n",
       "      <td>1CH</td>\n",
       "      <td>POR</td>\n",
       "      <td>1CH_SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGID...</td>\n",
       "      <td>SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGIDA FI...</td>\n",
       "      <td>zefania</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>historical</td>\n",
       "      <td>historical</td>\n",
       "      <td>old</td>\n",
       "      <td>Historical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839925</th>\n",
       "      <td>2.0</td>\n",
       "      <td>b.1CH.001.003</td>\n",
       "      <td>Enoque, Matusalém, Lameque,</td>\n",
       "      <td>1CH</td>\n",
       "      <td>POR</td>\n",
       "      <td>1CH_SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGID...</td>\n",
       "      <td>SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGIDA FI...</td>\n",
       "      <td>zefania</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>historical</td>\n",
       "      <td>historical</td>\n",
       "      <td>old</td>\n",
       "      <td>Historical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839926</th>\n",
       "      <td>3.0</td>\n",
       "      <td>b.1CH.001.004</td>\n",
       "      <td>Noé, Sem, Cão e Jafé.</td>\n",
       "      <td>1CH</td>\n",
       "      <td>POR</td>\n",
       "      <td>1CH_SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGID...</td>\n",
       "      <td>SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGIDA FI...</td>\n",
       "      <td>zefania</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>historical</td>\n",
       "      <td>historical</td>\n",
       "      <td>old</td>\n",
       "      <td>Historical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839927</th>\n",
       "      <td>4.0</td>\n",
       "      <td>b.1CH.001.005</td>\n",
       "      <td>Os filhos de Jafé foram: Gomer, Magogue, Madai...</td>\n",
       "      <td>1CH</td>\n",
       "      <td>POR</td>\n",
       "      <td>1CH_SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGID...</td>\n",
       "      <td>SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGIDA FI...</td>\n",
       "      <td>zefania</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>historical</td>\n",
       "      <td>historical</td>\n",
       "      <td>old</td>\n",
       "      <td>Historical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         books_labels.csv             id  \\\n",
       "1839923               0.0  b.1CH.001.001   \n",
       "1839924               1.0  b.1CH.001.002   \n",
       "1839925               2.0  b.1CH.001.003   \n",
       "1839926               3.0  b.1CH.001.004   \n",
       "1839927               4.0  b.1CH.001.005   \n",
       "\n",
       "                                                      text codebook language  \\\n",
       "1839923                                  ADÃO, Sete, Enos,      1CH      POR   \n",
       "1839924                          Cainã, Maalaleel, Jerede,      1CH      POR   \n",
       "1839925                        Enoque, Matusalém, Lameque,      1CH      POR   \n",
       "1839926                              Noé, Sem, Cão e Jafé.      1CH      POR   \n",
       "1839927  Os filhos de Jafé foram: Gomer, Magogue, Madai...      1CH      POR   \n",
       "\n",
       "                                            book_file_name  \\\n",
       "1839923  1CH_SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGID...   \n",
       "1839924  1CH_SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGID...   \n",
       "1839925  1CH_SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGID...   \n",
       "1839926  1CH_SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGID...   \n",
       "1839927  1CH_SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGID...   \n",
       "\n",
       "                                     file_name_translation   source    year  \\\n",
       "1839923  SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGIDA FI...  zefania  1995.0   \n",
       "1839924  SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGIDA FI...  zefania  1995.0   \n",
       "1839925  SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGIDA FI...  zefania  1995.0   \n",
       "1839926  SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGIDA FI...  zefania  1995.0   \n",
       "1839927  SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGIDA FI...  zefania  1995.0   \n",
       "\n",
       "              genre genre-multilabel testament    division  \n",
       "1839923  historical       historical       old  Historical  \n",
       "1839924  historical       historical       old  Historical  \n",
       "1839925  historical       historical       old  Historical  \n",
       "1839926  historical       historical       old  Historical  \n",
       "1839927  historical       historical       old  Historical  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portuguese_df.sort_values(by=['file_name_translation', 'id']).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Estas', 'son', 'las', 'palabras', 'de', 'Amós', ',', 'que', 'era', 'un', 'pastor', 'de', 'Tecoa']\n"
     ]
    }
   ],
   "source": [
    "# Clean, format, and tokenize Spanish texts\n",
    "spanish_list_clean = [regex.sub(r'\\([a-zA-z0-9]\\)', '', item) for item in spanish_list]\n",
    "spanish_string = ''.join(spanish_list_clean)\n",
    "spanish_list_clean = spanish_string.split('.')\n",
    "spanish_list_clean = [item.strip(' ') for item in spanish_list_clean]\n",
    "spanish_corpus = [nltk.tokenize.casual_tokenize(item) for item in spanish_list_clean][:1000]\n",
    "print(spanish_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Portuguese texts\n",
    "# https://stackoverflow.com/questions/17424182/extracting-all-rows-from-pandas-dataframe-that-have-certain-value-in-a-specific\n",
    "portuguese_df = df[df['language'] == 'POR']['text']\n",
    "portuguese_list = portuguese_df.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No', 'segundo', 'ano', 'do', 'rei', 'Dario', ',', 'no', 'sexto', 'mês', ',', 'no', 'primeiro', 'dia', 'do', 'mês', ',', 'veio', 'a', 'palavra', 'do', 'Senhor', ',', 'por', 'intermédio', 'do', 'profeta', 'Ageu', ',', 'a', 'Zorobabel', ',', 'governador', 'de', 'Judá', ',', 'filho', 'de', 'Sealtiel', ',', 'e', 'a', 'Josué', ',', 'o', 'sumo', 'sacerdote', ',', 'filho', 'de', 'Jeozadaque', ',', 'dizendo', ':', 'Assim', 'fala', 'o', 'Senhor', 'dos', 'exércitos', ',', 'dizendo', ':', 'Este', 'povo', 'diz', ':', 'Não', 'veio', 'ainda', 'o', 'tempo', ',', 'o', 'tempo', 'de', 'se', 'edificar', 'a', 'casa', 'do', 'Senhor']\n"
     ]
    }
   ],
   "source": [
    "# Format and tokenize portuguese texts\n",
    "for i, item in enumerate(portuguese_list):\n",
    "    if type(item) == float:\n",
    "        portuguese_list.pop(i)\n",
    "portuguese_string = ''.join(portuguese_list)\n",
    "portuguese_list = portuguese_string.split('.')\n",
    "portuguese_list = [item.strip(' ') for item in portuguese_list]\n",
    "portuguese_corpus = [nltk.tokenize.casual_tokenize(item) for item in portuguese_list][:1000]\n",
    "print(portuguese_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garbage collect old variables\n",
    "del df\n",
    "del spanish_string, portuguese_string, spanish_list, spanish_list_clean, portuguese_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3312 7304 4441\n",
      "['No', 'segundo', 'ano', 'do', 'rei', 'Dario', ',', 'no', 'sexto', 'mês', ',', 'no', 'primeiro', 'dia', 'do', 'mês', ',', 'veio', 'a', 'palavra', 'do', 'Senhor', ',', 'por', 'intermédio', 'do', 'profeta', 'Ageu', ',', 'a', 'Zorobabel', ',', 'governador', 'de', 'Judá', ',', 'filho', 'de', 'Sealtiel', ',', 'e', 'a', 'Josué', ',', 'o', 'sumo', 'sacerdote', ',', 'filho', 'de', 'Jeozadaque', ',', 'dizendo', ':', 'Assim', 'fala', 'o', 'Senhor', 'dos', 'exércitos', ',', 'dizendo', ':', 'Este', 'povo', 'diz', ':', 'Não', 'veio', 'ainda', 'o', 'tempo', ',', 'o', 'tempo', 'de', 'se', 'edificar', 'a', 'casa', 'do', 'Senhor'] ['[START]', 'No', 'segundo', 'ano', 'do', 'rei', 'Dario', ',', 'no', 'sexto', 'mês', ',', 'no', 'primeiro', 'dia', 'do', 'mês', ',', 'veio', 'a', 'palavra', 'do', 'Senhor', ',', 'por', 'intermédio', 'do', 'profeta', 'Ageu', ',', 'a', 'Zorobabel', ',', 'governador', 'de', 'Judá', ',', 'filho', 'de', 'Sealtiel', ',', 'e', 'a', 'Josué', ',', 'o', 'sumo', 'sacerdote', ',', 'filho', 'de', 'Jeozadaque', ',', 'dizendo', ':', 'Assim', 'fala', 'o', 'Senhor', 'dos', 'exércitos', ',', 'dizendo', ':', 'Este', 'povo', 'diz', ':', 'Não', 'veio', 'ainda', 'o', 'tempo', ',', 'o', 'tempo', 'de', 'se', 'edificar', 'a', 'casa', 'do', 'Senhor', '[END]']\n"
     ]
    }
   ],
   "source": [
    "# Prepare vocabulary\n",
    "spanish_input_texts, spanish_target_texts = [], []\n",
    "portuguese_input_texts, portuguese_target_texts = [], []\n",
    "spanish_vocabulary = set()\n",
    "portuguese_vocabulary = set()\n",
    "start_token = '[START]'\n",
    "stop_token = '[END]'\n",
    "unknown_token = '[UNK]'\n",
    "spanish_vocabulary.add(start_token)\n",
    "spanish_vocabulary.add(stop_token)\n",
    "spanish_vocabulary.add(unknown_token)\n",
    "portuguese_vocabulary.add(start_token)\n",
    "portuguese_vocabulary.add(stop_token)\n",
    "portuguese_vocabulary.add(unknown_token)\n",
    "\n",
    "for spanish_input_text in spanish_corpus:\n",
    "    spanish_target_text = [start_token] + spanish_input_text + [stop_token]\n",
    "    spanish_input_texts.append(spanish_input_text)\n",
    "    spanish_target_texts.append(spanish_target_text)\n",
    "    for char in spanish_target_text:\n",
    "        if char not in spanish_vocabulary:\n",
    "            spanish_vocabulary.add(char)\n",
    "\n",
    "for portuguese_input_text in portuguese_corpus:\n",
    "    portuguese_target_text = [start_token] + portuguese_input_text + [stop_token]\n",
    "    portuguese_input_texts.append(portuguese_input_text)\n",
    "    portuguese_target_texts.append(portuguese_target_text)\n",
    "    for char in portuguese_target_text:\n",
    "        if char not in portuguese_vocabulary:\n",
    "            portuguese_vocabulary.add(char)\n",
    "\n",
    "unified_vocabulary = spanish_vocabulary.union(portuguese_vocabulary)\n",
    "\n",
    "print(len(spanish_vocabulary), len(unified_vocabulary), len(portuguese_vocabulary))\n",
    "print(portuguese_input_texts[0], portuguese_target_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish vocabulary\n",
    "spanish_vocabulary = sorted(spanish_vocabulary)\n",
    "portuguese_vocabulary = sorted(portuguese_vocabulary)\n",
    "\n",
    "# Define maxima\n",
    "spanish_vocab_size = len(spanish_vocabulary)\n",
    "portuguese_vocab_size = len(portuguese_vocabulary)\n",
    "unified_vocab_size = len(unified_vocabulary)\n",
    "max_spanish_seq_length = max([len(txt) for txt in spanish_target_texts])\n",
    "max_portuguese_seq_length = max([len(txt) for txt in portuguese_target_texts])\n",
    "max_unified_seq_length = max(max_spanish_seq_length, max_portuguese_seq_length)\n",
    "\n",
    "# Create indicies\n",
    "spanish_token_index = dict([(token, i+1) for i, token in\n",
    "                          enumerate(spanish_vocabulary)])\n",
    "portuguese_token_index = dict([(token, i+1) for i, token in\n",
    "                          enumerate(portuguese_vocabulary)])\n",
    "unified_token_index = dict([(token, i+1) for i, token in\n",
    "                          enumerate(unified_vocabulary)])\n",
    "reverse_spanish_token_index = dict([(i, token) for token, i in\n",
    "                          spanish_token_index.items()])\n",
    "reverse_portuguese_token_index = dict([(i, token) for token, i in\n",
    "                          portuguese_token_index.items()])\n",
    "reverse_unified_token_index = dict([(i, token) for token, i in\n",
    "                          unified_token_index.items()])\n",
    "reverse_unified_token_index[0] = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No', 'segundo', 'ano', 'do', 'rei', 'Dario', ',', 'no', 'sexto', 'mês', ',', 'no', 'primeiro', 'dia', 'do', 'mês', ',', 'veio', 'a', 'palavra', 'do', 'Senhor', ',', 'por', 'intermédio', 'do', 'profeta', 'Ageu', ',', 'a', 'Zorobabel', ',', 'governador', 'de', 'Judá', ',', 'filho', 'de', 'Sealtiel', ',', 'e', 'a', 'Josué', ',', 'o', 'sumo', 'sacerdote', ',', 'filho', 'de', 'Jeozadaque', ',', 'dizendo', ':', 'Assim', 'fala', 'o', 'Senhor', 'dos', 'exércitos', ',', 'dizendo', ':', 'Este', 'povo', 'diz', ':', 'Não', 'veio', 'ainda', 'o', 'tempo', ',', 'o', 'tempo', 'de', 'se', 'edificar', 'a', 'casa', 'do', 'Senhor', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[START]', 'No', 'segundo', 'ano', 'do', 'rei', 'Dario', ',', 'no', 'sexto', 'mês', ',', 'no', 'primeiro', 'dia', 'do', 'mês', ',', 'veio', 'a', 'palavra', 'do', 'Senhor', ',', 'por', 'intermédio', 'do', 'profeta', 'Ageu', ',', 'a', 'Zorobabel', ',', 'governador', 'de', 'Judá', ',', 'filho', 'de', 'Sealtiel', ',', 'e', 'a', 'Josué', ',', 'o', 'sumo', 'sacerdote', ',', 'filho', 'de', 'Jeozadaque', ',', 'dizendo', ':', 'Assim', 'fala', 'o', 'Senhor', 'dos', 'exércitos', ',', 'dizendo', ':', 'Este', 'povo', 'diz', ':', 'Não', 'veio', 'ainda', 'o', 'tempo', ',', 'o', 'tempo', 'de', 'se', 'edificar', 'a', 'casa', 'do', 'Senhor', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['Estas', 'son', 'las', 'palabras', 'de', 'Amós', ',', 'que', 'era', 'un', 'pastor', 'de', 'Tecoa', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[START]', 'Estas', 'son', 'las', 'palabras', 'de', 'Amós', ',', 'que', 'era', 'un', 'pastor', 'de', 'Tecoa', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert sentences to numpy arrays\n",
    "spanish_encoder_input_data = np.zeros((len(spanish_input_texts), max_spanish_seq_length),\n",
    "                               dtype='int32')\n",
    "spanish_decoder_input_data = np.zeros((len(spanish_input_texts), max_spanish_seq_length),\n",
    "                               dtype='int32')\n",
    "spanish_decoder_target_data = np.zeros((len(spanish_input_texts), max_spanish_seq_length),\n",
    "                               dtype='int32')\n",
    "\n",
    "portuguese_encoder_input_data = np.zeros((len(portuguese_input_texts), max_portuguese_seq_length),\n",
    "                               dtype='int32')\n",
    "portuguese_decoder_input_data = np.zeros((len(portuguese_input_texts), max_portuguese_seq_length),\n",
    "                               dtype='int32')\n",
    "portuguese_decoder_target_data = np.zeros((len(portuguese_input_texts), max_portuguese_seq_length),\n",
    "                               dtype='int32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(spanish_input_texts, spanish_target_texts)):\n",
    "    for t, token in enumerate(input_text):\n",
    "        spanish_encoder_input_data[\n",
    "            i, t] = unified_token_index[token]\n",
    "    for t, token in enumerate(target_text):\n",
    "        spanish_decoder_input_data[\n",
    "            i, t] = unified_token_index[token]\n",
    "        if t > 0:\n",
    "            spanish_decoder_target_data[i, t - 1] = spanish_token_index[token]\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(portuguese_input_texts, portuguese_target_texts)):\n",
    "    for t, token in enumerate(input_text):\n",
    "        portuguese_encoder_input_data[\n",
    "            i, t] = unified_token_index[token]\n",
    "    for t, token in enumerate(target_text):\n",
    "        portuguese_decoder_input_data[\n",
    "            i, t] = unified_token_index[token]\n",
    "        if t > 0:\n",
    "            portuguese_decoder_target_data[i, t - 1] = portuguese_token_index[token]\n",
    "\n",
    "print([reverse_unified_token_index[value] for value in portuguese_encoder_input_data[0]])\n",
    "print([reverse_unified_token_index[value] for value in portuguese_decoder_input_data[0]])\n",
    "print([reverse_unified_token_index[value] for value in spanish_encoder_input_data[0]])\n",
    "print([reverse_unified_token_index[value] for value in spanish_decoder_input_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3673, 3435, 6804, ...,    0,    0,    0],\n",
       "       [5964, 6812, 3160, ...,    0,    0,    0],\n",
       "       [7058,    8, 4694, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [1331,  185, 7158, ...,    0,    0,    0],\n",
       "       [6652, 6523, 6804, ...,    0,    0,    0],\n",
       "       [5447, 5227,  362, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display some input\n",
    "spanish_encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 120\n",
    "num_neurons = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from customized file\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 22:45:35.186462: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2024-04-10 22:45:35.186489: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-04-10 22:45:35.186494: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-04-10 22:45:35.186675: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-10 22:45:35.186693: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Define the encoder and two decoders\n",
    "encoder = Encoder(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    vocab_size=unified_vocab_size,\n",
    "    dropout_rate=dropout_rate)\n",
    "\n",
    "spanish_decoder = Decoder(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    vocab_size=unified_vocab_size,\n",
    "    dropout_rate=dropout_rate)\n",
    "\n",
    "portuguese_decoder = Decoder(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    vocab_size=unified_vocab_size,\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transformers from the encoder and decoder\n",
    "spanish_transformer = ComposedTransformer(encoder, spanish_decoder, spanish_vocab_size)\n",
    "portuguese_transformer = ComposedTransformer(encoder, portuguese_decoder, portuguese_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "spanish_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "portuguese_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile both models\n",
    "spanish_transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=spanish_optimizer,\n",
    "    metrics=[masked_accuracy])\n",
    "portuguese_transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=portuguese_optimizer,\n",
    "    metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "spanish_dataset = tf.data.Dataset.from_tensor_slices(((spanish_encoder_input_data, spanish_decoder_input_data), spanish_decoder_target_data))\n",
    "portuguese_dataset = tf.data.Dataset.from_tensor_slices(((spanish_encoder_input_data, spanish_decoder_input_data), spanish_decoder_target_data))\n",
    "spanish_batched_dataset = spanish_dataset.batch(batch_size)\n",
    "portuguese_batched_dataset = portuguese_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"composed_transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"composed_transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder (\u001b[38;5;33mEncoder\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (\u001b[38;5;33mDecoder\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Give a summary of the transformer\n",
    "spanish_transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'global_self_attention' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'encoder_layer' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'causal_self_attention' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'decoder_layer' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 8.1614 - masked_accuracy: 3.4620e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'causal_self_attention_4' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'decoder_layer_4' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 8.4214 - masked_accuracy: 1.0084e-04\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 8.1060 - masked_accuracy: 6.5290e-04\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 8.3729 - masked_accuracy: 1.6113e-04\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 7.9876 - masked_accuracy: 0.0111\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 8.2771 - masked_accuracy: 0.0089\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 7.8619 - masked_accuracy: 0.0553\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 8.1781 - masked_accuracy: 0.0598\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 7.7667 - masked_accuracy: 0.0614\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 8.0966 - masked_accuracy: 0.0608\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 7.6948 - masked_accuracy: 0.0608\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 8.0252 - masked_accuracy: 0.0608\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 7.6261 - masked_accuracy: 0.0608\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 7.9507 - masked_accuracy: 0.0608\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 7.5505 - masked_accuracy: 0.0608\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 7.8669 - masked_accuracy: 0.0608\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 7.4653 - masked_accuracy: 0.0608\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 7.7723 - masked_accuracy: 0.0608\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 7.3698 - masked_accuracy: 0.0614\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 7.6671 - masked_accuracy: 0.0608\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 7.2639 - masked_accuracy: 0.0622\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 7.5523 - masked_accuracy: 0.0608\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 7.1480 - masked_accuracy: 0.0621\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 7.4287 - masked_accuracy: 0.0608\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 7.0230 - masked_accuracy: 0.0640\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 7.2967 - masked_accuracy: 0.0623\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 6.8902 - masked_accuracy: 0.0752\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 7.1572 - masked_accuracy: 0.0682\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 6.7530 - masked_accuracy: 0.0891\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 7.0133 - masked_accuracy: 0.0728\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 6.6157 - masked_accuracy: 0.1059\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 6.8672 - masked_accuracy: 0.0866\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 6.4791 - masked_accuracy: 0.1168\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 6.7196 - masked_accuracy: 0.0985\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 6.3444 - masked_accuracy: 0.1288\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 6.5738 - masked_accuracy: 0.1095\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 6.2130 - masked_accuracy: 0.1396\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 6.4321 - masked_accuracy: 0.1214\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 6.0858 - masked_accuracy: 0.1525\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 6.2979 - masked_accuracy: 0.1341\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 5.9636 - masked_accuracy: 0.1616\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 6.1662 - masked_accuracy: 0.1443\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 5.8442 - masked_accuracy: 0.1736\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 6.0399 - masked_accuracy: 0.1578\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 5.7259 - masked_accuracy: 0.1896\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 5.9199 - masked_accuracy: 0.1695\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 5.6109 - masked_accuracy: 0.1999\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 5.8040 - masked_accuracy: 0.1842\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 5.4972 - masked_accuracy: 0.2106\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 5.6914 - masked_accuracy: 0.1945\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 5.3813 - masked_accuracy: 0.2231\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 5.5767 - masked_accuracy: 0.2041\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 5.2665 - masked_accuracy: 0.2336\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 5.4618 - masked_accuracy: 0.2178\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 5.1535 - masked_accuracy: 0.2450\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 5.3514 - masked_accuracy: 0.2273\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 5.0428 - masked_accuracy: 0.2575\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 5.2405 - masked_accuracy: 0.2365\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 4.9455 - masked_accuracy: 0.2629\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 5.1343 - masked_accuracy: 0.2522\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 4.8576 - masked_accuracy: 0.2677\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 5.0455 - masked_accuracy: 0.2561\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 4.7737 - masked_accuracy: 0.2719\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 4.9337 - masked_accuracy: 0.2681\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 4.6659 - masked_accuracy: 0.2846\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 4.8153 - masked_accuracy: 0.2804\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 4.5590 - masked_accuracy: 0.2949\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 4.7213 - masked_accuracy: 0.2869\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 4.4742 - masked_accuracy: 0.2996\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 4.6262 - masked_accuracy: 0.2939\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 4.3860 - masked_accuracy: 0.3069\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 4.5309 - masked_accuracy: 0.3007\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 4.3038 - masked_accuracy: 0.3155\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 4.4260 - masked_accuracy: 0.3093\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 4.1949 - masked_accuracy: 0.3225\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 4.3213 - masked_accuracy: 0.3157\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 4.1128 - masked_accuracy: 0.3260\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 4.2354 - masked_accuracy: 0.3199\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 4.0654 - masked_accuracy: 0.3285\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 4.1752 - masked_accuracy: 0.3260\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.9958 - masked_accuracy: 0.3376\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 4.0809 - masked_accuracy: 0.3351\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 3.9012 - masked_accuracy: 0.3487\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.9913 - masked_accuracy: 0.3420\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 3.8180 - masked_accuracy: 0.3560\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.9136 - masked_accuracy: 0.3527\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.7591 - masked_accuracy: 0.3614\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.8583 - masked_accuracy: 0.3539\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 3.7209 - masked_accuracy: 0.3632\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.7682 - masked_accuracy: 0.3676\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 3.6159 - masked_accuracy: 0.3740\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.6789 - masked_accuracy: 0.3727\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 3.5243 - masked_accuracy: 0.3930\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.5848 - masked_accuracy: 0.3888\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 3.4369 - masked_accuracy: 0.4034\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.5488 - masked_accuracy: 0.3837\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 3.3653 - masked_accuracy: 0.4094\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.4555 - masked_accuracy: 0.3949\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 3.3058 - masked_accuracy: 0.4162\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.3718 - masked_accuracy: 0.4081\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.2293 - masked_accuracy: 0.4283\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 3.2905 - masked_accuracy: 0.4224\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 3.1755 - masked_accuracy: 0.4364\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.2159 - masked_accuracy: 0.4306\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.0886 - masked_accuracy: 0.4416\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.1485 - masked_accuracy: 0.4394\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 3.0181 - masked_accuracy: 0.4552\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.1072 - masked_accuracy: 0.4419\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.9558 - masked_accuracy: 0.4635\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 3.0245 - masked_accuracy: 0.4507\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.8848 - masked_accuracy: 0.4707\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 2.9597 - masked_accuracy: 0.4624\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.8126 - masked_accuracy: 0.4832\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 2.8901 - masked_accuracy: 0.4717\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.7640 - masked_accuracy: 0.4872\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 2.8271 - masked_accuracy: 0.4806\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.7310 - masked_accuracy: 0.4905\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 2.7743 - masked_accuracy: 0.4871\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.6482 - masked_accuracy: 0.5035\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 2.7269 - masked_accuracy: 0.4929\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.6250 - masked_accuracy: 0.5154\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 2.6493 - masked_accuracy: 0.5072\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.5371 - masked_accuracy: 0.5175\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 2.5557 - masked_accuracy: 0.5210\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.4097 - masked_accuracy: 0.5448\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 2.4393 - masked_accuracy: 0.5430\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.2995 - masked_accuracy: 0.5668\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 2.3547 - masked_accuracy: 0.5513\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.2287 - masked_accuracy: 0.5733\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 2.2837 - masked_accuracy: 0.5708\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.1767 - masked_accuracy: 0.5865\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.2214 - masked_accuracy: 0.5726\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.0912 - masked_accuracy: 0.5930\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.1678 - masked_accuracy: 0.5782\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.9983 - masked_accuracy: 0.6169\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.1140 - masked_accuracy: 0.5996\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.9462 - masked_accuracy: 0.6270\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.0450 - masked_accuracy: 0.6006\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.9158 - masked_accuracy: 0.6265\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.9534 - masked_accuracy: 0.6210\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.8569 - masked_accuracy: 0.6402\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.8921 - masked_accuracy: 0.6259\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.7380 - masked_accuracy: 0.6603\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.7967 - masked_accuracy: 0.6404\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.6199 - masked_accuracy: 0.6860\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.6886 - masked_accuracy: 0.6692\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.5343 - masked_accuracy: 0.7058\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.5938 - masked_accuracy: 0.6911\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.4677 - masked_accuracy: 0.7227\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.5065 - masked_accuracy: 0.7052\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.3969 - masked_accuracy: 0.7326\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.4444 - masked_accuracy: 0.7250\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.3383 - masked_accuracy: 0.7491\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.3725 - masked_accuracy: 0.7343\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.2635 - masked_accuracy: 0.7652\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.3115 - masked_accuracy: 0.7554\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.2011 - masked_accuracy: 0.7872\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.2563 - masked_accuracy: 0.7769\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.1155 - masked_accuracy: 0.8048\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 9s/step - loss: 1.1748 - masked_accuracy: 0.7821\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.0452 - masked_accuracy: 0.8185\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.1006 - masked_accuracy: 0.8064\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.9620 - masked_accuracy: 0.8425\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.9941 - masked_accuracy: 0.8312\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.8799 - masked_accuracy: 0.8647\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.9274 - masked_accuracy: 0.8525\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.8167 - masked_accuracy: 0.8719\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.8593 - masked_accuracy: 0.8646\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.7585 - masked_accuracy: 0.8940\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.8051 - masked_accuracy: 0.8807\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.6924 - masked_accuracy: 0.9046\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.7306 - masked_accuracy: 0.9013\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.6206 - masked_accuracy: 0.9282\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.6529 - masked_accuracy: 0.9191\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.5544 - masked_accuracy: 0.9375\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.5888 - masked_accuracy: 0.9312\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.5037 - masked_accuracy: 0.9475\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 0.5563 - masked_accuracy: 0.9402\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.4815 - masked_accuracy: 0.9513\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.5175 - masked_accuracy: 0.9449\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.4517 - masked_accuracy: 0.9560\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.4762 - masked_accuracy: 0.9510\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.4047 - masked_accuracy: 0.9653\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.4266 - masked_accuracy: 0.9634\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.3601 - masked_accuracy: 0.9695\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.3747 - masked_accuracy: 0.9714\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.3182 - masked_accuracy: 0.9762\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.3252 - masked_accuracy: 0.9789\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.2820 - masked_accuracy: 0.9815\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.2839 - masked_accuracy: 0.9859\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.2374 - masked_accuracy: 0.9870\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.2482 - masked_accuracy: 0.9878\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.2020 - masked_accuracy: 0.9898\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.2235 - masked_accuracy: 0.9901\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.1838 - masked_accuracy: 0.9899\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.1986 - masked_accuracy: 0.9919\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.1668 - masked_accuracy: 0.9904\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 0.1671 - masked_accuracy: 0.9936\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.1494 - masked_accuracy: 0.9906\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.1532 - masked_accuracy: 0.9929\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.1385 - masked_accuracy: 0.9910\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.1373 - masked_accuracy: 0.9942\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.1269 - masked_accuracy: 0.9915\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.1309 - masked_accuracy: 0.9929\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.1124 - masked_accuracy: 0.9917\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.1194 - masked_accuracy: 0.9939\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0972 - masked_accuracy: 0.9932\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.1015 - masked_accuracy: 0.9953\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0938 - masked_accuracy: 0.9914\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.0947 - masked_accuracy: 0.9941\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0879 - masked_accuracy: 0.9915\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.0906 - masked_accuracy: 0.9940\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.0757 - masked_accuracy: 0.9935\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 0.0948 - masked_accuracy: 0.9920\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.0756 - masked_accuracy: 0.9923\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 0.0996 - masked_accuracy: 0.9895\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.0686 - masked_accuracy: 0.9930\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 0.1011 - masked_accuracy: 0.9875\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0690 - masked_accuracy: 0.9921\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.0896 - masked_accuracy: 0.9907\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0724 - masked_accuracy: 0.9905\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.0779 - masked_accuracy: 0.9916\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0655 - masked_accuracy: 0.9912\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.0584 - masked_accuracy: 0.9948\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0639 - masked_accuracy: 0.9905\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0615 - masked_accuracy: 0.9933\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0669 - masked_accuracy: 0.9888\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0504 - masked_accuracy: 0.9947\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m963s\u001b[0m 64s/step - loss: 0.0591 - masked_accuracy: 0.9903\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m990s\u001b[0m 66s/step - loss: 0.0369 - masked_accuracy: 0.9971\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0467 - masked_accuracy: 0.9923\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0333 - masked_accuracy: 0.9966\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0393 - masked_accuracy: 0.9938\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0392 - masked_accuracy: 0.9953\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0401 - masked_accuracy: 0.9927\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 33s/step - loss: 0.0494 - masked_accuracy: 0.9929\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.0345 - masked_accuracy: 0.9934\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.0484 - masked_accuracy: 0.9929\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.0330 - masked_accuracy: 0.9938\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.0537 - masked_accuracy: 0.9923\n"
     ]
    }
   ],
   "source": [
    "# Train the transformers\n",
    "tf.config.run_functions_eagerly(True)\n",
    "for i in range(epochs):\n",
    "    spanish_transformer.fit(spanish_batched_dataset)\n",
    "    portuguese_transformer.fit(portuguese_batched_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_transformer.save_weights('./models/spanish120epoch.weights.h5')\n",
    "portuguese_transformer.save_weights('./models/portuguese120epoch.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a translator\n",
    "class Translator(tf.Module):\n",
    "  def __init__(self, transformer, input_token_index, reverse_input_token_index, output_token_index, reverse_output_token_index):\n",
    "    self.transformer = transformer\n",
    "    self.input_token_index = input_token_index\n",
    "    self.output_token_index = output_token_index\n",
    "    self.reverse_input_token_index = reverse_input_token_index\n",
    "    self.reverse_output_token_index = reverse_output_token_index\n",
    "\n",
    "  def __call__(self, sentence, max_length):\n",
    "    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
    "    # assert isinstance(sentence, tf.Tensor)\n",
    "    # if len(sentence.shape) == 0:\n",
    "    #   sentence = sentence[tf.newaxis]\n",
    "    tokenized_sentence = nltk.tokenize.casual_tokenize(sentence)\n",
    "    sentence_tensor = np.zeros(max_length, dtype='int64')\n",
    "    for t, token in enumerate(tokenized_sentence):\n",
    "      sentence_tensor[t] = self.input_token_index[token]\n",
    "    #sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
    "\n",
    "    encoder_input = sentence_tensor[tf.newaxis]\n",
    "    \n",
    "    # As the output language is English, initialize the output with the\n",
    "    # English `[START]` token.\n",
    "    #start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "    start = self.input_token_index['[START]']\n",
    "    end = self.output_token_index['[END]']\n",
    "\n",
    "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "    # dynamic-loop can be traced by `tf.function`.\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "    for i in tf.range(max_length):\n",
    "      output = tf.transpose(output_array.stack())[tf.newaxis]\n",
    "      predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "      # Select the last token from the `seq_len` dimension.\n",
    "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "      # Concatenate the `predicted_id` to the output which is given to the\n",
    "      # decoder as its input.\n",
    "      output_array = output_array.write(i+1, predicted_id[0][0])\n",
    "\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    output = tf.transpose(output_array.stack())\n",
    "    # The output shape is `(1, tokens)`. https://www.tensorflow.org/api_docs/python/tf/cast\n",
    "    text = [item.numpy() for item in output]  # Shape: `()`.\n",
    "\n",
    "    # `tf.function` prevents us from using the attention_weights that were\n",
    "    # calculated on the last iteration of the loop.\n",
    "    # So, recalculate them outside the loop.\n",
    "    # self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "    # attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'global_self_attention' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'encoder_layer' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'causal_self_attention' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'decoder_layer' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'causal_self_attention_4' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'decoder_layer_4' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Test the translator\n",
    "sentence = 'Estas son las palabras de Amós , que era un pastor de Tecoa'\n",
    "spanish_translator = Translator(spanish_transformer, unified_token_index, reverse_unified_token_index, spanish_token_index, reverse_spanish_token_index)\n",
    "portuguese_translator = Translator(portuguese_transformer, unified_token_index, reverse_unified_token_index, portuguese_token_index, reverse_portuguese_token_index)\n",
    "spanish_translated_text = spanish_translator(\n",
    "    sentence,15)\n",
    "portuguese_translated_text = portuguese_translator(\n",
    "    sentence,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3519, 299, 2642, 7, 2023, 2023, 1271, 7, 1271, 7, 1271, 7, 1271, 2642, 1271, 2642]\n",
      "[3519, 299, 2963, 1271, 2963, 1271, 7, 1271, 7, 1271, 7, 1560, 2264, 7, 2264, 7]\n"
     ]
    }
   ],
   "source": [
    "# Print the raw lists\n",
    "print(spanish_translated_text)\n",
    "print(portuguese_translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estas que , las las de , de , de , de que de que \n",
      "puseram-se Gadi muitíssimo carvalhos muitíssimo carvalhos : carvalhos : carvalhos : covas fechou : fechou : "
     ]
    }
   ],
   "source": [
    "# Print the sentences themselves\n",
    "for i in range(20):\n",
    "    try:\n",
    "        print(reverse_spanish_token_index[spanish_translated_text[i]], end=' ')\n",
    "    except Exception:\n",
    "        continue\n",
    "print()\n",
    "for i in range(20):\n",
    "    try:\n",
    "        print(reverse_portuguese_token_index[portuguese_translated_text[i]], end=' ')\n",
    "    except Exception:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
