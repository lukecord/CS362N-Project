{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary and residual imports\n",
    "import nltk\n",
    "from nltk.corpus import udhr\n",
    "import keras\n",
    "from nltk import FreqDist\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import regex\n",
    "import nltk.tokenize.casual\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset: (from https://huggingface.co/datasets/versae/bibles)\n",
    "# https://stackoverflow.com/questions/39263929/how-can-i-read-tar-gz-file-using-pandas-read-csv-with-gzip-compression-option\n",
    "df = pd.read_csv('books_labels.tar.gz', compression='gzip', header=0, sep=',', quotechar='\"', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Spanish texts\n",
    "# https://stackoverflow.com/questions/17424182/extracting-all-rows-from-pandas-dataframe-that-have-certain-value-in-a-specific\n",
    "#spanish_df = df[df['language'] == 'SPA']['text']\n",
    "#spanish_list = spanish_df.to_list() # https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://ioflood.com/blog/dataframe-to-list-pandas/%23:~:text%3DIt%2520is%2520utilized%2520with%2520the,tolist()%2520.%26text%3DIn%2520the%2520example%2520above%252C%2520we,convert%2520it%2520into%2520a%2520list.&ved=2ahUKEwiP3deMnLSFAxVD4ckDHUo7BsEQFnoECA4QAw&usg=AOvVaw3GgfIVdsVo9Dxul02uata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/05/btfxjzv52_369qlngpc4v8fc0000gn/T/ipykernel_13259/44389737.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  spanish_df = spanish_df.sort_values(by=['file_name_translation', 'id'])[df['file_name_translation'] == 'Nueva Biblia de las Américas (NBLA)'] # https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.reddit.com/r/Reformed/comments/xoye82/most_accurate_spanish_bible_translations_spanish/&ved=2ahUKEwj7qbSKtbuFAxV4IEQIHbm6B_UQFnoECCwQAQ&usg=AOvVaw2YZJ8eHalVoloIoaaATROH\n"
     ]
    }
   ],
   "source": [
    "# Select Spanish texts\n",
    "# https://stackoverflow.com/questions/17424182/extracting-all-rows-from-pandas-dataframe-that-have-certain-value-in-a-specific\n",
    "spanish_df = df[df['language'] == 'SPA']\n",
    "portuguese_df = df[df['language'] == 'POR']\n",
    "\n",
    "spanish_df = spanish_df.sort_values(by=['file_name_translation', 'id'])[df['file_name_translation'] == 'Nueva Biblia de las Américas (NBLA)'] # https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.reddit.com/r/Reformed/comments/xoye82/most_accurate_spanish_bible_translations_spanish/&ved=2ahUKEwj7qbSKtbuFAxV4IEQIHbm6B_UQFnoECCwQAQ&usg=AOvVaw2YZJ8eHalVoloIoaaATROH\n",
    "spanish_df = spanish_df.set_index('id')\n",
    "spanish_df = spanish_df.drop(columns=['books_labels.csv', 'codebook', 'language', 'book_file_name', 'file_name_translation', 'source', 'year', 'genre', 'genre-multilabel', 'testament', 'division'])\n",
    "#spanish_list = spanish_df['text'].to_list() # https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://ioflood.com/blog/dataframe-to-list-pandas/%23:~:text%3DIt%2520is%2520utilized%2520with%2520the,tolist()%2520.%26text%3DIn%2520the%2520example%2520above%252C%2520we,convert%2520it%2520into%2520a%2520list.&ved=2ahUKEwiP3deMnLSFAxVD4ckDHUo7BsEQFnoECA4QAw&usg=AOvVaw3GgfIVdsVo9Dxul02uata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/05/btfxjzv52_369qlngpc4v8fc0000gn/T/ipykernel_13259/2818949468.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  portuguese_df = portuguese_df.sort_values(by=['file_name_translation', 'id'])[df['file_name_translation'] == 'SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGIDA FIEL (1753_1995))']\n"
     ]
    }
   ],
   "source": [
    "portuguese_df = portuguese_df.sort_values(by=['file_name_translation', 'id'])[df['file_name_translation'] == 'SF_2009-01-20_POR_ACF_(PORTUGUESE CORRIGIDA FIEL (1753_1995))']\n",
    "portuguese_df = portuguese_df.set_index('id')\n",
    "portuguese_df = portuguese_df.drop(columns=['books_labels.csv', 'codebook', 'language', 'book_file_name', 'file_name_translation', 'source', 'year', 'genre', 'genre-multilabel', 'testament', 'division'])\n",
    "\n",
    "#portuguese_list = portuguese_df['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean, format, and tokenize Spanish texts\n",
    "#spanish_list = [regex.sub(r'\\([a-zA-z0-9]\\)', '', item) for item in spanish_list]\n",
    "# spanish_string = ''.join(spanish_list_clean)\n",
    "# spanish_list_clean = spanish_string.split('.')\n",
    "# spanish_list_clean = [item.strip(' ') for item in spanish_list_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b.1CH.001.001</th>\n",
       "      <td>(A)Adán, Set, Enós,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b.1CH.001.002</th>\n",
       "      <td>Cainán, Mahalaleel, Jared,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b.1CH.001.003</th>\n",
       "      <td>Enoc, Matusalén, Lamec,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b.1CH.001.004</th>\n",
       "      <td>Noé, Sem, Cam y Jafet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b.1CH.001.005</th>\n",
       "      <td>(B)Los hijos de Jafet fueron Gomer, Magog, Mad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            text\n",
       "id                                                              \n",
       "b.1CH.001.001                                (A)Adán, Set, Enós,\n",
       "b.1CH.001.002                         Cainán, Mahalaleel, Jared,\n",
       "b.1CH.001.003                            Enoc, Matusalén, Lamec,\n",
       "b.1CH.001.004                             Noé, Sem, Cam y Jafet.\n",
       "b.1CH.001.005  (B)Los hijos de Jafet fueron Gomer, Magog, Mad..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish: (C)Y Mizrayim fue el padre del pueblo de Ludim, Anamim, Lehabim, Neftuhim, Portuguese: E Mizraim gerou aos ludeus e aos anameus e aos leabeus e aos naftueus,\n",
      "Spanish: Canaán fue el padre de Sidón su primogénito, y de Het, Portuguese: E Canaã gerou a Sidom seu primogênito, e a Hete,\n"
     ]
    }
   ],
   "source": [
    "spanish_df = spanish_df.rename(columns={'text':'spanish_text'})\n",
    "portuguese_df = portuguese_df.rename(columns={'text':'portuguese_text'})\n",
    "\n",
    "multi_df = spanish_df.join(portuguese_df)\n",
    "multi_df = multi_df.dropna()\n",
    "\n",
    "print(\"Spanish:\", multi_df['spanish_text']['b.1CH.001.011'],\"Portuguese:\", multi_df['portuguese_text']['b.1CH.001.011'])\n",
    "\n",
    "spanish_list = multi_df['spanish_text'].to_list()\n",
    "portuguese_list = multi_df['portuguese_text'].to_list()\n",
    "\n",
    "print('Spanish:', spanish_list[11], 'Portuguese:', portuguese_list[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adán', ',', 'Set', ',', 'Enós', ',']\n",
      "['ADÃO', ',', 'Sete', ',', 'Enos', ',']\n"
     ]
    }
   ],
   "source": [
    "# Format and tokenize lists to make corpi\n",
    "spanish_list = [regex.sub(r'\\([a-zA-z0-9]\\)', '', item) for item in spanish_list]\n",
    "spanish_corpus = [nltk.tokenize.casual_tokenize(item) for item in spanish_list]\n",
    "print(spanish_corpus[0])\n",
    "portuguese_corpus = [nltk.tokenize.casual_tokenize(item) for item in portuguese_list]\n",
    "print(portuguese_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Portuguese texts\n",
    "# https://stackoverflow.com/questions/17424182/extracting-all-rows-from-pandas-dataframe-that-have-certain-value-in-a-specific\n",
    "# portuguese_df = df[df['language'] == 'POR']['text']\n",
    "# portuguese_list = portuguese_df.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garbage collect old variables\n",
    "del df\n",
    "del spanish_list, portuguese_list, spanish_df, portuguese_df, multi_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26936 50891 29203\n",
      "['[SPSTART]', 'Adán', ',', 'Set', ',', 'Enós', ',', '[END]'] ['[POSTART]', 'ADÃO', ',', 'Sete', ',', 'Enos', ','] ['ADÃO', ',', 'Sete', ',', 'Enos', ',', '[END]']\n"
     ]
    }
   ],
   "source": [
    "# Prepare vocabulary\n",
    "spanish_encoder_texts, spanish_input_texts, spanish_target_texts = [], [], []\n",
    "portuguese_encoder_texts, portuguese_input_texts, portuguese_target_texts = [], [], []\n",
    "spanish_vocabulary = set()\n",
    "portuguese_vocabulary = set()\n",
    "spanish_start_token = '[SPSTART]'\n",
    "portuguese_start_token = '[POSTART]'\n",
    "stop_token = '[END]'\n",
    "unknown_token = '[UNK]'\n",
    "pad_token = '[PAD]'\n",
    "spanish_vocabulary.add(spanish_start_token)\n",
    "spanish_vocabulary.add(stop_token)\n",
    "spanish_vocabulary.add(unknown_token)\n",
    "spanish_vocabulary.add(pad_token)\n",
    "portuguese_vocabulary.add(portuguese_start_token)\n",
    "portuguese_vocabulary.add(stop_token)\n",
    "portuguese_vocabulary.add(unknown_token)\n",
    "portuguese_vocabulary.add(pad_token)\n",
    "\n",
    "for spanish_text in spanish_corpus:\n",
    "    spanish_encoder_texts.append([spanish_start_token] + spanish_text + [stop_token])\n",
    "    spanish_input_texts.append([spanish_start_token] + spanish_text)\n",
    "    spanish_target_texts.append(spanish_text + [stop_token])\n",
    "    for char in spanish_text:\n",
    "        if char not in spanish_vocabulary:\n",
    "            spanish_vocabulary.add(char)\n",
    "\n",
    "for portuguese_text in portuguese_corpus:\n",
    "    portuguese_encoder_texts.append([portuguese_start_token] + portuguese_text + [stop_token])\n",
    "    portuguese_input_texts.append([portuguese_start_token] + portuguese_text)\n",
    "    portuguese_target_texts.append(portuguese_text + [stop_token])\n",
    "    for char in portuguese_text:\n",
    "        if char not in portuguese_vocabulary:\n",
    "            portuguese_vocabulary.add(char)\n",
    "\n",
    "unified_vocabulary = spanish_vocabulary.union(portuguese_vocabulary)\n",
    "\n",
    "print(len(spanish_vocabulary), len(unified_vocabulary), len(portuguese_vocabulary))\n",
    "print(spanish_encoder_texts[0], portuguese_input_texts[0], portuguese_target_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 114 109\n"
     ]
    }
   ],
   "source": [
    "# Finish vocabulary\n",
    "spanish_vocabulary = sorted(spanish_vocabulary)\n",
    "portuguese_vocabulary = sorted(portuguese_vocabulary)\n",
    "\n",
    "# Define maxima\n",
    "spanish_vocab_size = len(spanish_vocabulary)\n",
    "portuguese_vocab_size = len(portuguese_vocabulary)\n",
    "unified_vocab_size = len(unified_vocabulary)\n",
    "max_spanish_seq_length = max([len(txt) for txt in spanish_target_texts])\n",
    "max_portuguese_seq_length = max([len(txt) for txt in portuguese_target_texts])\n",
    "max_unified_seq_length = max(max_spanish_seq_length, max_portuguese_seq_length)\n",
    "\n",
    "# Create indicies\n",
    "spanish_token_index = dict([(token, i) for i, token in\n",
    "                          enumerate(spanish_vocabulary)])\n",
    "portuguese_token_index = dict([(token, i) for i, token in\n",
    "                          enumerate(portuguese_vocabulary)])\n",
    "unified_token_index = dict([(token, i) for i, token in\n",
    "                          enumerate(unified_vocabulary)])\n",
    "reverse_spanish_token_index = dict([(i, token) for token, i in\n",
    "                          spanish_token_index.items()])\n",
    "reverse_portuguese_token_index = dict([(i, token) for token, i in\n",
    "                          portuguese_token_index.items()])\n",
    "reverse_unified_token_index = dict([(i, token) for token, i in\n",
    "                          unified_token_index.items()])\n",
    "print(max_spanish_seq_length, max_unified_seq_length, max_portuguese_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_indices(texts, token_index, max_seq_length, pad_token='[PAD]'):\n",
    "    data = np.zeros((len(texts), max_seq_length),\n",
    "                    dtype='int32')\n",
    "    for i, text in enumerate(texts):\n",
    "        for t, token in enumerate(text):\n",
    "            data[i, t] = token_index[token]\n",
    "        for t in range(len(text), max_seq_length):\n",
    "            data[i, t] = token_index[pad_token]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[POSTART]', 'ADÃO', ',', 'Sete', ',', 'Enos', ',', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[POSTART]', 'ADÃO', ',', 'Sete', ',', 'Enos', ',', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['ADÃO', ',', 'Sete', ',', 'Enos', ',', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[SPSTART]', 'Adán', ',', 'Set', ',', 'Enós', ',', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[SPSTART]', 'Adán', ',', 'Set', ',', 'Enós', ',', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['Adán', ',', 'Set', ',', 'Enós', ',', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert sentences to numpy arrays\n",
    "spanish_encoder_input_data = convert_text_to_indices(spanish_encoder_texts, unified_token_index, max_spanish_seq_length+1)\n",
    "spanish_decoder_input_data = convert_text_to_indices(spanish_input_texts, unified_token_index, max_spanish_seq_length)\n",
    "spanish_decoder_target_data = convert_text_to_indices(spanish_target_texts, unified_token_index, max_spanish_seq_length)\n",
    "\n",
    "portuguese_encoder_input_data = convert_text_to_indices(portuguese_encoder_texts, unified_token_index, max_portuguese_seq_length+1)\n",
    "portuguese_decoder_input_data = convert_text_to_indices(portuguese_input_texts, unified_token_index, max_portuguese_seq_length)\n",
    "portuguese_decoder_target_data = convert_text_to_indices(portuguese_target_texts, unified_token_index, max_portuguese_seq_length)\n",
    "\n",
    "# spanish_encoder_input_data = np.zeros((len(spanish_encoder_texts), max_spanish_seq_length),\n",
    "#                                dtype='int32')\n",
    "# spanish_decoder_input_data = np.zeros((len(spanish_input_texts), max_spanish_seq_length),\n",
    "#                                dtype='int32')\n",
    "# spanish_decoder_target_data = np.zeros((len(spanish_target_texts), max_spanish_seq_length),\n",
    "#                                dtype='int32')\n",
    "\n",
    "# portuguese_encoder_input_data = np.zeros((len(portuguese_encoder_texts), max_portuguese_seq_length),\n",
    "#                                dtype='int32')\n",
    "# portuguese_decoder_input_data = np.zeros((len(portuguese_input_texts), max_portuguese_seq_length),\n",
    "#                                dtype='int32')\n",
    "# portuguese_decoder_target_data = np.zeros((len(portuguese_target_texts), max_portuguese_seq_length),\n",
    "#                                dtype='int32')\n",
    "\n",
    "\n",
    "\n",
    "# for i, (input_text, target_text) in enumerate(zip(spanish_input_texts, spanish_target_texts)):\n",
    "#     for t, token in enumerate(input_text):\n",
    "#         spanish_encoder_input_data[\n",
    "#             i, t] = unified_token_index[token]\n",
    "#     for t, token in enumerate(target_text):\n",
    "#         spanish_decoder_input_data[\n",
    "#             i, t] = unified_token_index[token]\n",
    "#         if t > 0:\n",
    "#             spanish_decoder_target_data[i, t - 1] = spanish_token_index[token]\n",
    "\n",
    "# for i, (input_text, target_text) in enumerate(zip(portuguese_input_texts, portuguese_target_texts)):\n",
    "#     for t, token in enumerate(input_text):\n",
    "#         portuguese_encoder_input_data[\n",
    "#             i, t] = unified_token_index[token]\n",
    "#     for t, token in enumerate(target_text):\n",
    "#         portuguese_decoder_input_data[\n",
    "#             i, t] = unified_token_index[token]\n",
    "#         if t > 0:\n",
    "#             portuguese_decoder_target_data[i, t - 1] = portuguese_token_index[token]\n",
    "\n",
    "print([reverse_unified_token_index[value] for value in portuguese_encoder_input_data[0]])\n",
    "print([reverse_unified_token_index[value] for value in portuguese_decoder_input_data[0]])\n",
    "print([reverse_unified_token_index[value] for value in portuguese_decoder_target_data[0]])\n",
    "print([reverse_unified_token_index[value] for value in spanish_encoder_input_data[0]])\n",
    "print([reverse_unified_token_index[value] for value in spanish_decoder_input_data[0]])\n",
    "print([reverse_unified_token_index[value] for value in spanish_decoder_target_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30283, 18216, 17907, ..., 40616, 40616, 40616],\n",
       "       [30283,  9217, 17907, ..., 40616, 40616, 40616],\n",
       "       [30283,  5487, 17907, ..., 40616, 40616, 40616],\n",
       "       ...,\n",
       "       [30283,  9154, 16042, ..., 40616, 40616, 40616],\n",
       "       [30283, 47221, 30766, ..., 40616, 40616, 40616],\n",
       "       [30283, 25911, 39241, ..., 40616, 40616, 40616]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display some input\n",
    "spanish_encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "num_neurons = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from customized file\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder and two decoders\n",
    "# encoder = Encoder(\n",
    "#     num_layers=num_layers,\n",
    "#     d_model=d_model,\n",
    "#     num_heads=num_heads,\n",
    "#     dff=dff,\n",
    "#     vocab_size=unified_vocab_size,\n",
    "#     dropout_rate=dropout_rate)\n",
    "\n",
    "# spanish_decoder = Decoder(\n",
    "#     num_layers=num_layers,\n",
    "#     d_model=d_model,\n",
    "#     num_heads=num_heads,\n",
    "#     dff=dff,\n",
    "#     vocab_size=unified_vocab_size,\n",
    "#     dropout_rate=dropout_rate)\n",
    "\n",
    "# portuguese_decoder = Decoder(\n",
    "#     num_layers=num_layers,\n",
    "#     d_model=d_model,\n",
    "#     num_heads=num_heads,\n",
    "#     dff=dff,\n",
    "#     vocab_size=unified_vocab_size,\n",
    "#     dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 08:02:13.084479: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2024-04-25 08:02:13.084501: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-04-25 08:02:13.084511: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-04-25 08:02:13.084527: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-25 08:02:13.084539: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Create the transformers from the encoder and decoder\n",
    "transformer = Transformer(num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=unified_vocab_size,\n",
    "    target_vocab_size=unified_vocab_size,\n",
    "    dropout_rate=dropout_rate)\n",
    "# spanish_transformer = ComposedTransformer(encoder, spanish_decoder, spanish_vocab_size)\n",
    "# portuguese_transformer = ComposedTransformer(encoder, portuguese_decoder, portuguese_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile both models\n",
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(((spanish_encoder_input_data[1000:2000], portuguese_decoder_input_data[1000:2000]), portuguese_decoder_target_data[1000:2000]))\n",
    "# batched_dataset = dataset.batch(batch_size)\n",
    "# spanish_dataset = tf.data.Dataset.from_tensor_slices(((spanish_encoder_input_data, spanish_decoder_input_data), spanish_decoder_target_data))\n",
    "# portuguese_dataset = tf.data.Dataset.from_tensor_slices(((spanish_encoder_input_data, spanish_decoder_input_data), spanish_decoder_target_data))\n",
    "# spanish_batched_dataset = spanish_dataset.batch(batch_size)\n",
    "# portuguese_batched_dataset = portuguese_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder (\u001b[38;5;33mEncoder\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (\u001b[38;5;33mDecoder\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Give a summary of the transformer\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a translator\n",
    "class Translator(tf.Module):\n",
    "  def __init__(self, transformer, token_index, reverse_token_index, input_start_char, output_start_char):\n",
    "    self.transformer = transformer\n",
    "    self.token_index = token_index\n",
    "    self.reverse_token_index = reverse_token_index\n",
    "    self.input_start_char = input_start_char\n",
    "    self.output_start_char = output_start_char\n",
    "\n",
    "  def __call__(self, sentence, max_length):\n",
    "    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
    "    # assert isinstance(sentence, tf.Tensor)\n",
    "    # if len(sentence.shape) == 0:\n",
    "    #   sentence = sentence[tf.newaxis]\n",
    "    tokenized_sentence = [self.input_start_char] + nltk.tokenize.casual_tokenize(sentence.lower()) + ['[END]']\n",
    "    sentence_tensor = np.zeros(len(tokenized_sentence), dtype='int64')\n",
    "    for t, token in enumerate(tokenized_sentence):\n",
    "      if token in self.token_index:\n",
    "        sentence_tensor[t] = self.token_index[token]\n",
    "      else:\n",
    "        sentence_tensor[t] = self.token_index['[UNK]']\n",
    "    #sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
    "\n",
    "    encoder_input = sentence_tensor[tf.newaxis]\n",
    "    \n",
    "    # As the output language is English, initialize the output with the\n",
    "    # English `[START]` token.\n",
    "    #start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "    start = self.token_index[self.output_start_char]\n",
    "    end = self.token_index['[END]']\n",
    "\n",
    "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "    # dynamic-loop can be traced by `tf.function`.\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "    for i in tf.range(max_length):\n",
    "      output = tf.transpose(output_array.stack())[tf.newaxis]\n",
    "      predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "      # Select the last token from the `seq_len` dimension.\n",
    "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "      # Concatenate the `predicted_id` to the output which is given to the\n",
    "      # decoder as its input.\n",
    "      output_array = output_array.write(i+1, predicted_id[0][0])\n",
    "\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    output = tf.transpose(output_array.stack())\n",
    "    # The output shape is `(1, tokens)`. https://www.tensorflow.org/api_docs/python/tf/cast\n",
    "    text = [self.reverse_token_index[item.numpy()] for item in output]  # Shape: `()`.\n",
    "\n",
    "    # `tf.function` prevents us from using the attention_weights that were\n",
    "    # calculated on the last iteration of the loop.\n",
    "    # So, recalculate them outside the loop.\n",
    "    # self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "    # attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import meteor_score\n",
    "import random\n",
    "def calculate_average_meteor(input_dataset, confirmation_dataset, translator, numtests, maxlen):\n",
    "    score = 0\n",
    "    i = 0\n",
    "    while i < numtests:\n",
    "        randindex = random.randrange(0, len(input_dataset))\n",
    "        if len(input_dataset[randindex]) <= maxlen:\n",
    "            expected = confirmation_dataset[randindex]\n",
    "            result = translator(' '.join(input_dataset[randindex]), len(confirmation_dataset[randindex]))\n",
    "            score += meteor_score.single_meteor_score(result[1:], expected)\n",
    "            i += 1\n",
    "    return score / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the transformers\n",
    "tf.config.run_functions_eagerly(True)\n",
    "# transformer.fit(batched_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28510 28510 28510\n",
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'global_self_attention' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'encoder_layer' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'causal_self_attention' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'decoder_layer' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m634s\u001b[0m 709ms/step - loss: 7.8491 - masked_accuracy: 0.6261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10454156775170706\n",
      "['[POSTART]', 'E', 'o', 'SENHOR', ',', 'e', 'o', 'SENHOR', ',', 'e', 'o', 'SENHOR', ',', 'e', 'o', 'SENHOR', ',', 'e', 'o', 'SENHOR', '.']\n",
      "Epoch 1:\n",
      "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 721ms/step - loss: 1.2354 - masked_accuracy: 0.8129\n",
      "0.28154190270406404\n",
      "['[POSTART]', 'O', 'anjo', 'no', 'dia', 'dos', 'céus', ',', 'e', 'os', 'céus', ',', 'e', 'a', 'terra', '.', '[END]']\n",
      "Epoch 2:\n",
      "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m638s\u001b[0m 716ms/step - loss: 0.9161 - masked_accuracy: 0.8532\n",
      "0.28364881580741746\n",
      "['[POSTART]', 'No', 'princípio', 'do', 'princípio', 'do', 'reino', 'dos', 'céus', 'e', 'a', 'terra', '.', '[END]']\n",
      "Epoch 3:\n",
      "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m637s\u001b[0m 715ms/step - loss: 0.7423 - masked_accuracy: 0.8745\n",
      "0.43708845359073206\n",
      "['[POSTART]', 'No', 'princípio', 'do', 'SENHOR', 'teu', 'deus', 'nos', 'céus', 'e', 'a', 'terra', '.', '[END]']\n",
      "Epoch 4:\n",
      "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m637s\u001b[0m 715ms/step - loss: 0.6419 - masked_accuracy: 0.8860\n",
      "0.49426857709562527\n",
      "['[POSTART]', 'No', 'ano', 'criou', 'o', 'monte', ',', 'e', 'a', 'terra', ',', 'e', 'a', 'terra', ';', '[END]']\n",
      "Epoch 5:\n",
      "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m638s\u001b[0m 716ms/step - loss: 0.5429 - masked_accuracy: 0.9004\n",
      "0.37428814929252635\n",
      "['[POSTART]', 'No', 'princípio', 'princípio', 'do', 'céu', 'criou', 'os', 'céus', 'e', 'a', 'terra', '.', '[END]']\n",
      "Epoch 6:\n",
      "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m635s\u001b[0m 713ms/step - loss: 0.4601 - masked_accuracy: 0.9130\n",
      "0.23881349206349203\n",
      "['[POSTART]', 'No', 'princípio', 'tem', 'o', 'princípio', ',', 'e', 'o', 'céu', 'e', 'a', 'terra', 'e', 'a', 'terra', '.', '[END]']\n"
     ]
    }
   ],
   "source": [
    "print(len(spanish_encoder_input_data), len(portuguese_decoder_input_data), len(portuguese_decoder_target_data))\n",
    "for t in range(7):\n",
    "    print(f\"Epoch {t}:\")\n",
    "    for i in range(1):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(((spanish_encoder_input_data, portuguese_decoder_input_data), portuguese_decoder_target_data))\n",
    "        dataset = dataset.shuffle(dataset.cardinality())\n",
    "        batched_dataset = dataset.batch(batch_size)\n",
    "        transformer.fit(batched_dataset)\n",
    "    sentence = 'En el principio Dios creó los cielos y la tierra.'\n",
    "    translator = Translator(transformer, unified_token_index, reverse_unified_token_index, '[SPSTART]', '[POSTART]')\n",
    "    print(calculate_average_meteor(spanish_corpus, portuguese_corpus, translator, 10, 20))\n",
    "    print(translator(sentence,20))\n",
    "    # sentence = 'Estas son las palabras de Amós, que era un pastor de Tecoa'\n",
    "    # translator = Translator(transformer, unified_token_index, reverse_unified_token_index, unified_token_index, reverse_unified_token_index)\n",
    "    # # portuguese_translator = Translator(portuguese_transformer, unified_token_index, reverse_unified_token_index, unified_token_index, reverse_unified_token_index)\n",
    "    # portuguese_translated_text = translator(\n",
    "    #     sentence,15)\n",
    "    # for i in portuguese_translated_text:\n",
    "    #     try:\n",
    "    #         print(reverse_unified_token_index[i], end=' ')\n",
    "    #     except Exception:\n",
    "    #         print(i)\n",
    "    #         continue\n",
    "        #transformer.save_weights(f'./models/spanish-portuguese_full_epoch{t}.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./translators/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./translators/1/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(transformer, export_dir=f'./translators/{t}')\n",
    "transformer = tf.saved_model.load(export_dir='./translators/0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luke/Documents/Code/CS362N Natural Language Processing/.venv/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:418: UserWarning: Skipping variable loading for optimizer 'adam', because it has 1 variables whereas the saved optimizer has 345 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# spanish_transformer.save_weights('./models/spanish120epoch.weights.h5')\n",
    "# portuguese_transformer.save_weights('./models/portuguese120epoch.weights.h5')\n",
    "#transformer.save_weights('./models/spanish-portuguese_full_epoch59.weights.h5')\n",
    "# transformer.save('./models/spanish-portuguese_full_test.keras') # https://discuss.tensorflow.org/t/save-a-tensorflow-model-with-a-transformer-layer/7221\n",
    "# transformer = tf.keras.models.load_model('./models/spanish-portuguese_full_test.keras') # https://discuss.tensorflow.org/t/save-a-tensorflow-model-with-a-transformer-layer/7221\n",
    "transformer.load_weights('./models/spanish-portuguese_full_epoch9.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a translator\n",
    "class Translator(tf.Module):\n",
    "  def __init__(self, transformer, input_token_index, reverse_input_token_index, output_token_index, reverse_output_token_index):\n",
    "    self.transformer = transformer\n",
    "    self.input_token_index = input_token_index\n",
    "    self.output_token_index = output_token_index\n",
    "    self.reverse_input_token_index = reverse_input_token_index\n",
    "    self.reverse_output_token_index = reverse_output_token_index\n",
    "\n",
    "  def __call__(self, sentence, max_length):\n",
    "    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
    "    # assert isinstance(sentence, tf.Tensor)\n",
    "    # if len(sentence.shape) == 0:\n",
    "    #   sentence = sentence[tf.newaxis]\n",
    "    tokenized_sentence = ['[SPSTART]'] + nltk.tokenize.casual_tokenize(sentence) + ['[END]']\n",
    "    sentence_tensor = np.zeros(max_length, dtype='int64')\n",
    "    for t, token in enumerate(tokenized_sentence):\n",
    "      sentence_tensor[t] = self.input_token_index[token]\n",
    "    #sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
    "\n",
    "    encoder_input = sentence_tensor[tf.newaxis]\n",
    "    \n",
    "    # As the output language is English, initialize the output with the\n",
    "    # English `[START]` token.\n",
    "    #start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "    start = self.input_token_index['[POSTART]']\n",
    "    end = self.output_token_index['[END]']\n",
    "\n",
    "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "    # dynamic-loop can be traced by `tf.function`.\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "    for i in tf.range(max_length):\n",
    "      output = tf.transpose(output_array.stack())[tf.newaxis]\n",
    "      predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "      # Select the last token from the `seq_len` dimension.\n",
    "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "      # Concatenate the `predicted_id` to the output which is given to the\n",
    "      # decoder as its input.\n",
    "      output_array = output_array.write(i+1, predicted_id[0][0])\n",
    "\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    output = tf.transpose(output_array.stack())\n",
    "    # The output shape is `(1, tokens)`. https://www.tensorflow.org/api_docs/python/tf/cast\n",
    "    text = [item.numpy() for item in output]  # Shape: `()`.\n",
    "\n",
    "    # `tf.function` prevents us from using the attention_weights that were\n",
    "    # calculated on the last iteration of the loop.\n",
    "    # So, recalculate them outside the loop.\n",
    "    # self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "    # attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_UserObject' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m translator \u001b[38;5;241m=\u001b[39m Translator(transformer, unified_token_index, reverse_unified_token_index, unified_token_index, reverse_unified_token_index)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# portuguese_translator = Translator(portuguese_transformer, unified_token_index, reverse_unified_token_index, unified_token_index, reverse_unified_token_index)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m portuguese_translated_text \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# portuguese_translated_text = portuguese_translator(\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     sentence,15)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[51], line 35\u001b[0m, in \u001b[0;36mTranslator.__call__\u001b[0;34m(self, sentence, max_length)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrange(max_length):\n\u001b[1;32m     34\u001b[0m   output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(output_array\u001b[38;5;241m.\u001b[39mstack())[tf\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m---> 35\u001b[0m   predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m   \u001b[38;5;66;03m# Select the last token from the `seq_len` dimension.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m   predictions \u001b[38;5;241m=\u001b[39m predictions[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:, :]  \u001b[38;5;66;03m# Shape `(batch_size, 1, vocab_size)`.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: '_UserObject' object is not callable"
     ]
    }
   ],
   "source": [
    "# Test the translator\n",
    "sentence = 'Estas son las palabras de Amós, que era un pastor de Tecoa'\n",
    "translator = Translator(transformer, unified_token_index, reverse_unified_token_index, unified_token_index, reverse_unified_token_index)\n",
    "# portuguese_translator = Translator(portuguese_transformer, unified_token_index, reverse_unified_token_index, unified_token_index, reverse_unified_token_index)\n",
    "portuguese_translated_text = translator(\n",
    "    sentence,15)\n",
    "# portuguese_translated_text = portuguese_translator(\n",
    "#     sentence,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43282, 26068, 21446, 8956, 2923, 1776, 44249, 33554, 44249, 14713, 13664, 19175, 40547, 48550, 16274, 19832]\n"
     ]
    }
   ],
   "source": [
    "# Print the raw lists\n",
    "#print(spanish_translated_text)\n",
    "print(portuguese_translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[POSTART] progresarán hendidura envidies composto ventre exacta intentaram exacta hendiste revés sereditas zelo secou Maldade - "
     ]
    }
   ],
   "source": [
    "# Print the sentences themselves\n",
    "# for i in range(20):\n",
    "#     try:\n",
    "#         print(reverse_spanish_token_index[spanish_translated_text[i]], end=' ')\n",
    "#     except Exception:\n",
    "#         continue\n",
    "# print()\n",
    "for i in portuguese_translated_text:\n",
    "    try:\n",
    "        print(reverse_unified_token_index[i], end=' ')\n",
    "    except Exception:\n",
    "        print(i)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2867"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del translator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
